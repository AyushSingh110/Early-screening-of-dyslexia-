{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "413292fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "from utils.predict import load_model\n",
    "from utils.preprocess import transform\n",
    "from utils.patchify import split_into_patches\n",
    "from utils.ocr import extract_text_from_image\n",
    "\n",
    "from language_model.language_risk import predict_language_risk\n",
    "from language_model.text_features import compute_raw_language_features\n",
    "\n",
    "import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d37960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings\n",
    "SCREENING_THRESHOLD = 0.4\n",
    "MIN_WORDS_LANGUAGE = 20\n",
    "\n",
    "MAX_PATCHES = 25                 \n",
    "RESIZE_IMAGE_TO = (1024, 1024)   \n",
    "HANDWRITING_SKIP_THRESHOLD = 0.15\n",
    "MAX_IMAGES_PER_CLASS = 50      \n",
    "TEST_DIR = \"data/dataset/test\"\n",
    "OCR_CACHE = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52470f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Handwriting model loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = load_model()  \n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Handwriting model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a39b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HANDWRITTING RISK\n",
    "def handwriting_risk_from_image(image: Image.Image) -> float:\n",
    "    image = image.resize(RESIZE_IMAGE_TO)\n",
    "    image_np = np.array(image)\n",
    "\n",
    "    patches = split_into_patches(\n",
    "        image_np,\n",
    "        patch_size=config.IMAGE_SIZE,\n",
    "        stride=config.IMAGE_SIZE\n",
    "    )\n",
    "\n",
    "    if len(patches) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    dyslexic_count = 0\n",
    "    patches = patches[:MAX_PATCHES]\n",
    "\n",
    "    for patch in patches:\n",
    "        patch_img = Image.fromarray(patch).convert(\"RGB\")\n",
    "        input_tensor = transform(patch_img).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prob = model(input_tensor).item()\n",
    "            if prob > 0.5:\n",
    "                dyslexic_count += 1\n",
    "\n",
    "    return dyslexic_count / len(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d354b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multimodal fusion\n",
    "def multimodal_fusion(handwriting_risk, language_risk):\n",
    "    if language_risk is None:\n",
    "        return handwriting_risk\n",
    "\n",
    "    # Language gate (only allow strong language evidence)\n",
    "    language_gate = max(\n",
    "        0.0,\n",
    "        min((language_risk - 0.5) / 0.5, 1.0)\n",
    "    )\n",
    "\n",
    "    final_risk = (\n",
    "        0.65 * handwriting_risk +\n",
    "        0.35 * language_gate * language_risk\n",
    "    )\n",
    "\n",
    "    # Conservative cap for neat handwriting\n",
    "    if handwriting_risk < 0.2:\n",
    "        final_risk = min(final_risk, 0.45)\n",
    "\n",
    "    return final_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88197e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Evaluating NORMAL (50 samples)\n",
      "Processed: A-100.png\n",
      "Processed: A-102.png\n",
      "Processed: A-104.png\n",
      "Processed: A-105.png\n",
      "Processed: A-107.png\n",
      "Processed: A-108.png\n",
      "Processed: A-109.png\n",
      "Processed: A-110.png\n",
      "Processed: A-112.png\n",
      "Processed: A-113.png\n",
      "Processed: A-114.png\n",
      "Processed: A-115.png\n",
      "Processed: A-116.png\n",
      "Processed: A-117.png\n",
      "Processed: A-118.png\n",
      "Processed: A-119.png\n",
      "Processed: A-120.png\n",
      "Processed: A-122.png\n",
      "Processed: A-123.png\n",
      "Processed: A-124.png\n",
      "Processed: A-126.png\n",
      "Processed: A-127.png\n",
      "Processed: A-128.png\n",
      "Processed: A-129.png\n",
      "Processed: A-130.png\n",
      "Processed: A-133.png\n",
      "Processed: A-135.png\n",
      "Processed: A-136.png\n",
      "Processed: A-137.png\n",
      "Processed: A-138.png\n",
      "Processed: A-139.png\n",
      "Processed: A-140.png\n",
      "Processed: A-141.png\n",
      "Processed: A-142.png\n",
      "Processed: A-144.png\n",
      "Processed: A-145.png\n",
      "Processed: A-146.png\n",
      "Processed: A-147.png\n",
      "Processed: A-148.png\n",
      "Processed: A-149.png\n",
      "Processed: A-150.png\n",
      "Processed: A-151.png\n",
      "Processed: A-152.png\n",
      "Processed: A-153.png\n",
      "Processed: A-154.png\n",
      "Processed: A-155.png\n",
      "Processed: A-156.png\n",
      "Processed: A-158.png\n",
      "Processed: A-159.png\n",
      "Processed: A-160.png\n",
      "\n",
      "üìÇ Evaluating DYSLEXIC (50 samples)\n",
      "Processed: 1_1.png\n",
      "Processed: 1_10.png\n",
      "Processed: 1_100.png\n",
      "Processed: 1_1000.png\n",
      "Processed: 1_1001.png\n",
      "Processed: 1_1002.png\n",
      "Processed: 1_1003.png\n",
      "Processed: 1_1004.png\n",
      "Processed: 1_1005.png\n",
      "Processed: 1_1006.png\n",
      "Processed: 1_1007.png\n",
      "Processed: 1_1008.png\n",
      "Processed: 1_1009.png\n",
      "Processed: 1_101.png\n",
      "Processed: 1_1010.png\n",
      "Processed: 1_1011.png\n",
      "Processed: 1_1012.png\n",
      "Processed: 1_1013.png\n",
      "Processed: 1_1014.png\n",
      "Processed: 1_1015.png\n",
      "Processed: 1_1016.png\n",
      "Processed: 1_1017.png\n",
      "Processed: 1_1018.png\n",
      "Processed: 1_1019.png\n",
      "Processed: 1_102.png\n",
      "Processed: 1_1020.png\n",
      "Processed: 1_1021.png\n",
      "Processed: 1_1022.png\n",
      "Processed: 1_1023.png\n",
      "Processed: 1_1024.png\n",
      "Processed: 1_1025.png\n",
      "Processed: 1_1026.png\n",
      "Processed: 1_1027.png\n",
      "Processed: 1_1028.png\n",
      "Processed: 1_1029.png\n",
      "Processed: 1_103.png\n",
      "Processed: 1_1030.png\n",
      "Processed: 1_1031.png\n",
      "Processed: 1_1032.png\n",
      "Processed: 1_1033.png\n",
      "Processed: 1_1034.png\n",
      "Processed: 1_1035.png\n",
      "Processed: 1_1036.png\n",
      "Processed: 1_1037.png\n",
      "Processed: 1_1038.png\n",
      "Processed: 1_1039.png\n",
      "Processed: 1_104.png\n",
      "Processed: 1_1040.png\n",
      "Processed: 1_1041.png\n",
      "Processed: 1_1042.png\n"
     ]
    }
   ],
   "source": [
    "#Evaluation fusion\n",
    "y_true = []\n",
    "y_pred_handwriting = []\n",
    "y_pred_multimodal = []\n",
    "\n",
    "for label_name in [\"normal\", \"dyslexic\"]:\n",
    "    label = 0 if label_name == \"normal\" else 1\n",
    "    folder = os.path.join(TEST_DIR, label_name)\n",
    "\n",
    "    files = os.listdir(folder)[:MAX_IMAGES_PER_CLASS]\n",
    "\n",
    "    print(f\"\\nEvaluating {label_name.upper()} ({len(files)} samples)\")\n",
    "\n",
    "    for fname in files:\n",
    "        img_path = os.path.join(folder, fname)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            # ----------------------\n",
    "            # Handwriting risk\n",
    "            # ----------------------\n",
    "            h_risk = handwriting_risk_from_image(image)\n",
    "\n",
    "            # ----------------------\n",
    "            # Language risk (GATED + CACHED)\n",
    "            # ----------------------\n",
    "            if h_risk < HANDWRITING_SKIP_THRESHOLD:\n",
    "                l_risk = None\n",
    "            else:\n",
    "                if fname in OCR_CACHE:\n",
    "                    text = OCR_CACHE[fname]\n",
    "                else:\n",
    "                    text = extract_text_from_image(image)\n",
    "                    OCR_CACHE[fname] = text\n",
    "\n",
    "                if len(text.split()) < MIN_WORDS_LANGUAGE:\n",
    "                    l_risk = None\n",
    "                else:\n",
    "                    raw_features = compute_raw_language_features(text)\n",
    "                    l_risk = (\n",
    "                        predict_language_risk(raw_features, debug=False)\n",
    "                        if raw_features is not None\n",
    "                        else None\n",
    "                    )\n",
    "\n",
    "            # ----------------------\n",
    "            # Fusion\n",
    "            # ----------------------\n",
    "            final_risk = multimodal_fusion(h_risk, l_risk)\n",
    "\n",
    "            # ----------------------\n",
    "            # Screening decisions\n",
    "            # ----------------------\n",
    "            y_true.append(label)\n",
    "            y_pred_handwriting.append(int(h_risk >= SCREENING_THRESHOLD))\n",
    "            y_pred_multimodal.append(int(final_risk >= SCREENING_THRESHOLD))\n",
    "\n",
    "            print(f\"Processed: {fname}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {fname}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "481bc295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.58      0.54        50\n",
      "           1       0.51      0.44      0.47        50\n",
      "\n",
      "    accuracy                           0.51       100\n",
      "   macro avg       0.51      0.51      0.51       100\n",
      "weighted avg       0.51      0.51      0.51       100\n",
      "\n",
      "[[29 21]\n",
      " [28 22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.58      0.54        50\n",
      "           1       0.51      0.44      0.47        50\n",
      "\n",
      "    accuracy                           0.51       100\n",
      "   macro avg       0.51      0.51      0.51       100\n",
      "weighted avg       0.51      0.51      0.51       100\n",
      "\n",
      "[[29 21]\n",
      " [28 22]]\n"
     ]
    }
   ],
   "source": [
    "#Results\n",
    "print(classification_report(y_true, y_pred_handwriting))\n",
    "print(confusion_matrix(y_true, y_pred_handwriting))\n",
    "\n",
    "print(classification_report(y_true, y_pred_multimodal))\n",
    "print(confusion_matrix(y_true, y_pred_multimodal))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyslexia_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
